%\documentclass[10pt,a4j]{utjarticle}
\documentclass[b5j,twoside,twocolumn]{utarticle}
%\documentclass[b5j,twoside]{utarticle}
%\documentclass[b5j,twoside,twocolumn]{utbook}
\setlength{\columnsep}{2zw}
\usepackage{bxpapersize}
\usepackage{pxrubrica}
\rubysetup{<hj>}
\pagestyle{headings}
\usepackage{endnotes}
\usepackage{multicol}
\renewcommand{\theendnote}{[後注\Roman{endnote}]}
\renewcommand{\thefootnote}{\arabic{footnote}}
\usepackage{pxftnright}
\usepackage{fancyhdr}
\setlength{\topmargin}{5mm} % ページ上部余白の設定（182mm x 257mmから計算）。
\addtolength{\topmargin}{-1in} % 初期設定の1インチ分を引いておく。
\setlength{\oddsidemargin}{21mm} % 同、奇数ページ左。
\addtolength{\oddsidemargin}{-1in}
\setlength{\evensidemargin}{17mm} % 同、偶数ページ左。
\addtolength{\evensidemargin}{-1in}
\setlength{\footskip}{-5mm}
%\setlength{\marginparwidth}{23mm}
%\setlength{\marginparsep}{5mm}
\usepackage{plext}
\setlength{\textwidth}{225mm} % 文書領域の幅（上下）。縦書と横書でパラメータ（width / height）の向きが変わる。
%\setlength{\textheight}{150mm} % 文書領域の幅（左右）
\makeatletter
\def\@cite#1#2{\rensuji{[{#1\if@tempswa , #2\fi}]}}%%
\def\@biblabel#1{\rensuji{[#1]}}%%%
\makeatother
\usepackage{enumerate}
\usepackage{braket}
\usepackage{url}
\usepackage[dvipdfmx]{graphicx}
\usepackage{float}
\usepackage{amsmath,amssymb}
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}
\usepackage{ascmac}
\usepackage{okumacro}
\usepackage{marginnote}
%\usepackage[top=15truemm,bottom=15truemm,left=20truemm,right=20truemm]{geometry}

%\renewcommand{\labelenumi}{(\Alph{enumi})}

\makeatletter
\@definecounter{yakuchu}
\@addtoreset{yakuchu}{document}% <--- depende on class file
\def\yakuchu{%
\@ifnextchar[\@xfootnote %]
{\stepcounter{yakuchu}%
\protected@xdef\@thefnmark{\theyakuchu}%
\@footnotemark\@footnotetext}}
\def\yakuchutext{%
\@ifnextchar [\@xfootnotenext %]
{\protected@xdef\@thefnmark{\theyakuchu}%
\@footnotetext}}
\def\yakuchumark{%
\@ifnextchar[\@xfootnotemark %]
{\stepcounter{yakuchu}%
\protected@xdef\@thefnmark{\theyakuchu}%
\@footnotemark}}
\makeatother

\pagestyle{fancy}

\title{-\tbaselineshift=4.0pt ブラックボックスと正義の政治-----ＡＩと差別}
\author{五十里 翔吾}
\date{\vspace{-5mm}}
\setcounter{page}{101}

\begin{document}
%\let\footnote=\endnote

\maketitle


\setlength{\footskip}{-2mm}
\lhead[]{【特集】「箱」}
\chead[]{}
\rhead[ブラックボックスと正義の政治------ＡＩと差別]{}
\lfoot[]{\thepage{}}
\cfoot[]{}
\rfoot[\thepage{}]{}

\thispagestyle{fancy}
\renewcommand{\footnoterule}{\noindent\rule{100mm}{0.3mm}\vskip2mm}


\section{はじめに：我々は中にいるか、外にいるか？}
ＡＩ（人工知能）の実装に用いられる機械学習の技術が、人間が行う差別を学習するという問題が指摘されている。
機械学習とは、与えられたデータからルールや判断基準、知識などを抽出する技術である。機械学習の技術は、我々の日常生活に欠かせない。例えば、検索エンジンや通信販売サイトのサジェストやスパムメールの仕分け、スマートフォンのカメラに備わる顔認識機能などその応用は多岐にわたる。


多くの場合、多様で複雑なデータを扱う機械学習システムはブラックボックス的になる。
例えば、その技術の一つであるディープラーニングにおいては、時に数千万以上ものパラメータがデータに適応するように調整されることで、学習が実現される。
プロ囲碁棋士を破った初のコンピュータ囲碁プログラムAlphaGoにはディープラーニングが用いられている。そして、AlphaGoがどのような戦略を立て、次の一手を選択するのかという決定プロセスを人間が理解することは不可能である\footnote{圧勝「囲碁ＡＩ」が露呈した人工知能の弱点 日本経済新聞web 2016/3/17 https://www.nikkei.com/article/DGXMZO98496540W6A310C1000000/}。


機械学習は人が開発した技術である。技術であるから、それを利用することができる。このように考えるならば、我々はこのブラックボックスの「外部」にあって、それを道具として利用しているのだと感じられよう。しかし、通販サイトのサジェストは、我々の購入履歴を学習し、「買いそうな」商品を提示する。また、画像に何が写っているのかを判断するシステムを開発する場合、学習に用いられるデータは、「ある画像」とそれが何の画像かという「正解」のラベルの組からなっており、その「正解」は人間が与えるのだ。そのような作業を仲介し、インターネット上の人々に外注するAmazon Mechanical Turkのようなシステムもある。また、あるサイトのログイン画面で「私は機械ではありません」ということを示すためにユーザが行う文字や画像の識別という簡単な作業が、学習用のデータを整備するのに活用されている\footnote{reCAPTCHA\\\url{https://www.google.com/recaptcha/intro/v3.html#creation-of-value}}。このような見方をすれば、機械学習というブラックボックスを、ある機能を果たす\ruby{「関数＝函数」}{ファンクション}という箱に見立てた場合、我々はその中にいるとも言えるのである。


しかし我々は、単に箱の中にいるだけではない。


例えば、「人がどれだけ信用できるかを予測する」システムを作りたいとしよう。このようなシステムは、クレジットカード会社や銀行も欲しがるだろう。すると、どんな情報を入力して学習させればいいだろうか。学歴だろうか？　職種だろうか？　このシステムは一体何を学習するのだろうか。学歴、職業、交友関係、住所、人種、ジェンダー、母語、年齢、障害など、個人に関わるあらゆる情報は、再生産される社会経済的基盤、そして文化的構造からは切り離すことはできない。このことを考慮するならば、以下のような事態が発生する可能性に思い至る。
\tbaselineshift=2.5pt------\tbaselineshift=4.0ptこのシステムは人々が行う差別を学習するかもしれない\tbaselineshift=2.5pt------\tbaselineshift=4.0pt「〇〇地区の人間は信用できない」「〇〇人は嘘をつく」
このブラックボックスは我々の社会が抱える「差別・偏見」が再現された「箱庭」となるかもしれない。すると、我々は庭師だ。


そんな事態は、現に進行している。本稿において、
２章では機械学習を用いたＡＩシステムにおける差別の実例を紹介する。
３章ではその原因を考察し、既存研究における対策を紹介し、その限界を明らかにする。
４章ではこの問題に取り組む上での包括的な指針を得るための予備的な考察を行う。


では、ブラックボックスの中で、外で、その狭間で、何が起こっているのか。まずは、この「箱」を\.外\.側から見てみることにしたい。



\section{\tbaselineshift=3.0pt機械学習による差別------箱庭の外側}
マイクロソフトの研究者で、ＡＩの社会的影響を研究する機関AI Now\footnote{https://ainowinstit ute.org/}の協同設立者ケイト・クロフォードは、機械学習における差別的効果(disparate impact)を\textbf{配分型}(allocative  harm)と\textbf{象徴型}(representation harm)の二種類に分類している\cite{kate}。配分型の差別とは、（機械学習を用いた）システムが、特定の人達に対してある機会やリソースへのアクセスを不公平に処理することである。象徴型の差別とは、システムが特定の集団のアイデンティティを貶めることに繋がる出力を行うことである。この分類は、後に４章で検討するが、哲学・あるいは政治学の用語として用いられる「再配分(redistribution)」と「承認(recognition)」という二つの概念と対応している。次節において、近年の事例から、それぞれのケースに当たるものを紹介する。



\subsection{機械学習による差別の事例}
\subsubsection*{配分型の差別}
\textbf{Amazonの人材採用ツール}\\
二〇一四年から、Amazonはより有能な労働者を採用するために、候補者の能力を予測するシステムを開発してきた。しかし、開発中の実験で、ソフトウェアエンジニアや一部の技術職において、ジェンダー間に不公平な見積もりを行っていたことが明らかになった\footnote{https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-AI-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}。そのため、Amazonはこのシステムを実際の採用プロセスには使用しなかった。具体的には、履歴書中の〝women〟や〝womens' chess club〟といった単語や、一部の女子大学名を低評価と結びつけていた。さらに、男性の履歴書に多く見られた〝executed〟や〝captured〟といった単語を高評価と結びつけていた。このシステムにおいては、過去十年間の採用候補者のデータを学習に用いており、候補者には男性が圧倒的に多かった。そのため、このようなジェンダー中立でない学習が行われたのである。（Amazonは二〇一七年以降技術職員のジェンダー比率を公表していないが、Google、Facebook、Microsoftにおいて、技術職にある労働者の\rensuji{80}％前後が男性である）
%https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ＡＩ-recruiting-tool-that-showed-bias-agＡＩnst-women-idUSKCN1MK08G


\textbf{再犯可能性予測システム(COMPAS)}\\
Northpointe社が開発したCOMPAS(Correctional Offender Management Profiling for Alternative Sanctions)は、犯罪者の再犯リスクや、犯罪的性格、薬物乱用のリスクなど\rensuji{20}以上の「犯罪兆候」を予測するシステムである。ニューヨークの裁判所では、二〇〇一年から導入されており、全米の多くの州で量刑に利用されている。二〇一二年に行われた保護観察下にある犯罪者のうち、\rensuji{71}％において再犯リスクを正確に予測できた。しかし、二〇一六年にフロリダ州の七〇〇〇人の犯罪者を対象に行われた実証実験\cite{compas}で、システムが人種に基づいて差別的な判定を下している可能性が明らかになった。「再犯リスク高」と予測され、\textbf{再犯をしなかった}白人の犯罪者は約\rensuji{24}％であった一方、同様の予測をされたアフリカ系アメリカ人のうち、約\rensuji{45}％が再犯をしなかった。逆に、「再犯リスク高」と予測され、\textbf{再犯した}白人の犯罪者は\rensuji{48}％ほどであった一方、同様の予測をされたアフリカ系アメリカ人のうち、\rensuji{28}％が再犯を犯した。
%Source: ProPublica analysis of data from Broward County, Fla.)

Northpointe社は、COMPASの入力変数に「人種」を採用していないと主張している。もしこれが真実であるとすれば、機械学習を用いたシステムが、入力変数以外から差別的な構造を学習している可能性がある。同様の結果が、個人の収入を予測するための学習を検討した研究によっても報告されている。国勢調査のデータセットを用いた学習を行った結果、「ジェンダー」の項目を入力変数に用いた場合よりも、用いなかった場合のほうが、男性を高収入、女性を低収入であると判別する割合が高くなった\cite{Calders2010}。

%http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.422.9495&rep=rep1&type=pdf
%https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
%[Angwin+16   [Calders+10]

\subsubsection*{象徴型の差別}
\textbf{瞬目検知システム}\\
Nikon S630デジタルカメラは、レンズに写った人の顔を認識し、瞬きを検知して使用者に知らせるというシステムを搭載している。二〇〇九年、このシステムがアジア人女性が常に「瞬きしている」と判別したというケースが報告されている\footnote{https://thesocietypages.org/socimages/2009/05/29/nikon-camera-says-asians-are-always-blinking/}。同様の問題が、ＨＰ社のウェブカメラHP Pavillionにおいても報告されている。Time.comはこの問題の原因は、学習アルゴリズムに内在すると指摘している。このシステムは入力画像から解像度を落として画像を処理しており、細い目と閉じた目を区別できなかったというのだ\footnote{https://petapixel.com/2010/01/22/racist-camera-phenomenon-explained-almost/}。

%https://petapixel.com/2010/01/22/racist-camera-phenomenon-explＡＩned-almost/
%https://thesocietypages.org/socimages/2009/05/29/nikon-camera-says-asians-are-always-blinking/
\textbf{オンライン広告提示システム}\\
google.comのような検索エンジンでは、入力した単語に対してページ上部に広告が表示されることがある。どのような広告が表示されるかは、機械学習を用いたシステムによって決定されている。instantcheckmate.comは、検索エンジンに広告を表示しているウェブサイトの一つだ。このサイトでは、アメリカ国民の様々な個人情報を名前と紐付けて記録しており、例えば〝Shogo IKARI〟というように人名を入力して検索した場合〝We Found: Shogo IKARI〟や〝Shogo IKARI, Arrested〟といった広告が表示される。ハーバード大学のラタニア・スウィーニーは二〇一二年に行った研究\cite{DBLP:journals/corr/abs-1301-6822}で、google.comとreuters.comにおいて、表示される広告を〝Found〝〝Located〟といった中立的な広告と〝Arrested〟広告に分け、名前と広告の文面がどのように結び付けられているのかを分析した。その結果、アフリカ系に多く見られる名前を検索した場合のほうがヨーロッパ系に多く見られる名前を検索した場合よりも〝Arrested〟という内容が表示される確率が高く、表示される広告の内容が人種という変数に従属していることが統計的に有意であることが示された。\\
%https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2208240

本節では、機械学習を用いたシステムが差別的な情報を出力した例を挙げた。これ以外にも、
機械翻訳において特定の単語が特定のジェンダーと結び付けられて訳出されたケース\footnote{三人称表現に性差のないトルコ語において、「[人]は[職業]である」という形式の文章が、例えば〝He is a doctor〟〝She is a nurse〟といった形で翻訳された。\url{https://www.facebook.com/photo.php?fbid=10154851496086949&set=a}}、チャットボットが差別的発言を学習したケース\footnote{Microsoftのチャットボット"Tay"が、人種差別を支持する発言を学習した。\\\url{https://www.theguardian.com/technology/2016/mar/24/tay-microsofts-ai-chatbot-gets-a-crash-course-in-racism-from-twitter}}など（どちらも象徴型）、様々な事態が報告されている。次章では、このような差別が発生する原因を分類し、既存の研究を紹介する。
%https://www.facebook.com/photo.php?fbid=10154851496086949&set=a
\section{箱庭の中の差別}
本稿の冒頭に述べたとおり、ディープラーニングに代表されるような、大量のデータを用いた機械学習システムの中身は基本的にはブラックボックスであり、詳細な情報の流れを追うことは難しい。しかし、差別が学習される原因を定性的な二つの過程に分けることは可能である。これを手がかりに、「暗箱」の\.内\.側で何が起こっているのかを明らかにしていきたい。
\subsection{差別が学習される原因}
%[Barocas+16] 
\textbf{（１）学習アルゴリズム}\\
~~瞬目検知システムにおいては、画像を処理するプロセスに問題が指摘された。
しかし、実際にはすべてのケースにおいて、学習アルゴリズムが差別の学習を直接的にであれ間接的にであれ誘導している可能性は否定できない。システムを構築するプロセスには、設計者による入出力変数の設計やモデルの選択など、様々な恣意性が入り込む余地がある。前章の国勢調査の例は、そのような選択が差別的な学習を誘導する可能性を示している。さらに、多くの学習モデルは、学習に用いるデータから多くのサンプルに共通する特徴を抽出するように開発されている。それゆえ、少数のグループにのみ共有される特徴はノイズとして無視するような性質を持っている。このような性質は、少数グループに対する差別が学習されてしまう原因となる。


\textbf{（２）学習に用いるデータ}\\
~~前章のAmazonの採用システムがその例である。瞬目検知システムにおいても、偏ったデータセットが用いられていた可能性がある。偏ったデータが学習に用いられる場合、その原因は二通りある。


（ａ）サンプリングバイアス\\
~~ジェームズ・ズーらが二〇一八年にNature誌に寄せた記事\cite{articleZou}によると、
スタンフォード大学のデータセットであるImageNetに含まれる画像のうち、\rensuji{45}％がアメリカから投稿されたものであった。アメリカの人口は世界の\rensuji{4}％を占めるに過ぎないが、人口の\rensuji{36}％を占めるインド、中国から投稿された画像はわずか\rensuji{3}％であった。このようなデータの偏りが、あるアルゴリズムにおいて生じた、アメリカの結婚式の写真に〝bride〟〝dress〟〝wedding〟といったラベルを与える一方で北インドの結婚式の写真には〝performance art〟あるいは〝costume〟というラベル付けをしてしまう\cite{shankar2017classification}、という事態を部分的に説明するとズーらは指摘する。また、自然言語のデータセットによく用いられるWikipediaの記述は、\rensuji{82}％が男性によって行われている\cite{inproceedings}。


（ｂ）ラベル付けにおける問題\\
~~さらに、ズーらは学習データセットにおける「正解」のラベル付けが、大学院生やクラウドソーシングを通じて集められる人々によって行われることが多いという点を指摘する。この過程で、ジェンダー、文化、人種に関する偏りが生じている可能性がある。例えば、Amazon Mechanical Turkに仲介されたワーカーの\rensuji{75}％がアメリカ人であることが分かっている\cite{Difallah2018DemographicsAD}。
%ネイチャ　https://www.nature.com/articles/d41586-018-05707-8#ref-CR2
%インド花嫁差別 https://arxiv.org/abs/1711.08536
%  メカニカルタークの国民比http://www.ipeirotis.com/wp-content/uploads/2017/12/wsdmf074-difallahA.pdf
%http://ibisml.org/ibis2018/files/2018/11/fukuchi.pdf


本節では、機械学習というブラックボックスの中に差別を再現した「箱庭」が生まれる原因を、\.内\.側から整理した。研究者達は、少しずつではあるがこの問題に取り組み始めている。次節では、そのような試みを簡単に紹介したい。


\subsection{公平性を保証する学習}
データに駆動された自律的な意思決定に差別が組み込まれてしまうことに対する懸念は、米政権らによる報告書でも繰り返し言及されてきた\cite{obama}\cite{obama2}。
% Big data: A report on algorithmic systems opportunity and civil rights
% John Podesta, Penny Pritzker, Ernest J. Moniz, John Holdren, and Jefrey Zients.
%Big data: Seizing opportunities and preserving values. Executive Office of the
%President, May 2014.

それに呼応する形で、機械学習の理論家たちは近年、公平な学習アルゴリズムを目指して、様々な理論を定式化している\footnote{本節をまとめるにあたって第\rensuji{21}回情報論的学習理論ワークショップ (IBIS 2018)における発表　福地一斗 「公平性に配慮した学習とその理論的課題」を参考にした。}。
以下に紹介するケースは、学習結果にある確率的な制約を持たせることを条件に加えている。以下に、「人材採用システム」を想定して、それぞれの定式化を整理する。

\textbf{Demographic parity\cite{hardt2016equality}}\\
%https://arxiv.org/pdf/1610.02413.pdf(前半)
~~この概念が表すのは、差別を生む可能性がある属性を「センシティブ属性」として特定し、その「センシティブ属性」のすべての属性値と、システムの「出力ラベル」（採用か非採用か）が無関係（独立）であるように学習せよ、という要求である。数式で表すと、出力ラベルを$\hat{Y}$、値１と０をそれぞれ「採用」「非採用」とし、センシティブ属性$S$の属性値$s, s' \in \Sigma$($\Sigma$は属性値の取り得る集合)としたとき、
\begin{eqnarray}
\mathbb{P}(\hat{Y} = 1|S=s) =  \mathbb{P}(\hat{Y} = 1|S=s')
\end{eqnarray}
が成立するように収束せよということになる。この制約は、アファーマティブ・アクションの実行とみなすことができる\cite{mouzannar2018fair}。
Demographic parity「データにバイアスが存在する」場合を想定し、不平等を是正するものだ。しかし、データに偏りがなく、センシティブ属性が採用に影響を与える必然性がある場合には、ある属性値を持つ者を、別の属性値を持つ者よりも優遇してしまう可能性がある。
%アファーマティブ・アクション https://arxiv.org/abs/1812.02952

\textbf{Equalized odds\cite{hardt2016equality}}\\
%https://ttic.uchicago.edu/~nati/Publications/HardtPriceSrebro2016.pdf
~~Equalized oddsは、データにバイアスが存在しないことを仮定したモデルである。このモデルが与える制約は、ある属性の元に「多数派」と「少数派」が存在した時に、「多数派」のみに適合した学習が起きないようにするものである。データのラベル（採用されたか否か）を$Y$とし、Yは値$０または１$を取るとすると、
\begin{eqnarray}
\mathbb{P}(\hat{Y} = 1|Y=y, S=s) =  \mathbb{P}(\hat{Y} = 1|Y=y, S=s')
\end{eqnarray}
と表すことができる。


\textbf{Lipschitz連続性による制約\cite{dwork2011fairness}}\\
~~以上のモデルは、差別が生じる要因をある「属性」に特定して、確率的に対処するものであった。一方で、Lipschitz連続性を応用したよる制約は「似た個人は似た結果を得られるべきだ」という主張を定式化する。すなわち、「ある属性値の違いのみによっては、出力ラベルに違いが生じない」ように制約をかける。数学的には、入力データ上での距離を$d$、出力ラベル上での距離を$D$、入力データを出力ラベルに対応させる写像を$M$とした時に、\\
\begin{eqnarray}
\forall x, y \in 入力データ,~~~~~D(Mx, My) \leq d(x, y)
\end{eqnarray}
と定式化される。現時点では、一般的な場合において数学的な最適性を満たす解を必ず発見する学習方法は見つかっていない。また、このモデルはデータの偏りによって生じる差別を助長する可能性もある。
%配分型だけ

以上に、機械学習の理論家によって提案されている、学習時に公平性を保証するためのモデルを挙げた。しかし、これらのモデルは、すべて二章の冒頭で示した分類における「配分型」の差別に対処するためのものである。すなわち、機械学習アルゴリズムが一般的に持つ、「多くのサンプルに共通する特徴を抽出する」あるいは「少数のサンプルにのみ現れる特徴は無視する」といった差別を助長しかねない性質を緩和することで、少数グループを軽視した「配分」が行われないようにするものである。


それでは、「象徴型」の差別にはどう向き合えばよいのだろうか。一見すると、「象徴型」の個々のケースにおいて、問題の解消のために設けるべきアルゴリズムやデータの制約は異なっているように思える。
よって、この論点に対して意義のある提案を行うためには、「機械学習における差別」を、既存の「正義の政治・哲学」の理論を踏まえて包括的に議論する必要があると考えられる。「機械学習における差別」に固有の特徴は存在するのだろうか。また、この問題について考察することが、既存の理論に何か示唆を与えはしないだろうか。
そこで次章は、「再配分」と「承認」を相互に還元できない概念として対置した理論家であるナンシー・フレイザーの論を手がかりに、「箱庭の中の差別」に対する是正策のための考察を行いたい。

\section{暗箱の政治学}
\subsection{改善策の組み換え}
%承認と再配分について整理
%機械学習における差別の問題は、道徳哲学的には、何の問題として理解すればよいだろうか。このとき方法論的には、この問題について包括的な対策の道筋を立てようとするならば、「配分」を基盤にした視点によって、承認をめぐる闘争を、分配闘争の副次的産物ないし単なる表面的現象と捉えることは適切ではない。同様に、「承認」を基盤に置き人格的アイデンティティの形成を善と把握する立場は適当ではないように思える。


機械学習システムにおいては、差別はその出力変数とシステムの使用環境に依存して、「再配分」、あるいは象徴的な差別、すなわち「誤承認」の問題として我々のもとに現れる。そして、それらシステムはその使用目的の差異により他のシステムに対して閉じており、あるシステム内での変化はそのシステム内にしか影響を与えない。
ゆえに状況の改善は、\.個\.々\.のシステムにおけるデータやアルゴリズムといった工学的構造、あるいはその背後に存在する社会経済的構造を問いに付して議論し続けることでしか達成されない。しかしながら、社会全体で問題に取り組むためには、このような試みを統合するための指針が必要である。


ナンシー・フレイザーは、「承認」と「再配分」を相互に還元することのできないカテゴリーであるとした上で、実践的な関心に基づきそれぞれに対する不正義を是正するための方策の統合を試みている。
\begin{quote}
それにも関わらず、事実上あらゆる事例で問題となっている毀損は、誤承認と不公正な配分との両方を含んでおり、それらは双方がどちらかの是正を介した間接的な策のみによっては完全に是正され得ないため、それぞれを独立して実践的な観点から考察しなければならない。したがって実践的には、事実上あらゆる事例の不正義を克服するためには再配分と承認の両方が必要である\footnote{タナー講義(\url{https://tannerlectures.utah.edu/_documents/a-to-z/f/Fraser98.pdf})を和文\cite{BB10510230}『再配分か承認か？ : 政治・哲学論争』 p.30を参考に訳したもの。}。
%https://tannerlectures.utah.edu/_documents/a-to-z/f/Fraser98.pdf p22　和文p30参考に和訳
\end{quote}
フレイザーは、「承認」と「再配分」を統合しようと試みる際に、それらが相互に緊張を伴うものであることを主張する。
\begin{quote}
先に論じたように、不公正配分に対する完全に納得のゆく改善策は、それ単独で考えると、誤承認を悪化させ得るし、逆に、誤承認に対する完全に納得のゆく改善策も、それ単独で考えた場合には、不公正配分を悪化させ得る。そして、それぞれの次元では不正義を正すことに成功した個別の改革も、それらが合わせて追求されると、相互に傷つけ合う可能性がある。それゆえに、必要なのは、不公正配分と誤承認を同時に是正し得るような統合されたアプローチなのである。（\cite{BB10510230}『再配分か承認か？: 政治・哲学論争』 p.100）
%p100
\end{quote}

この緊張に対する注目は、機械学習システムにおける不正義を考える上で不可欠である。なぜなら、差別が問題となり得るシステムは、公の場において運用されるものであり、その上、是正を行うためには差別を引き起こしている属性的な要因を特定する必要がある。よって、何らかの是正策が検討された場合には、その仕様が公に知られることにより「誤承認」の拡大が生じる可能性が否定できない。
%例えば、画像認識システムによって「人間ではない」と判断された者の尊厳は、それ以降そのような出力が二度と行われなかったとしても、何ら取り戻されるものではないし、それが公になることでさらなる悲劇が生じる可能性がある。


以上のような問題意識のもと、フレイザーは\textbf{統合の基本姿勢}として、「改善策の組み換え」を提案する。
\begin{quote}
その一つを私は\.改\.善\.策\.の\.組\.み\.換\.えと名付ける。このことが意味しているのは、正義のある次元に関わる改善策を正義の他の次元に係る不正義を正すために用いるということ、したがって、
配分に関わる改善策を誤承認を是正するために使用し、不公正配分を是正するために承認に関わる改善策を使用することである。（同書p.101）
%p101
\end{quote}
フレイザーは、「改善策の組み換え」という戦略と同時に、さまざまな改革が集団の「境界」に与え得る影響に対して自覚的である必要性（境界戦略）を訴える。これらの概念は、実質的な戦略を編みだす媒体となるものだとフレイザーは述べている。


次節において、これらの提案を踏まえて、「機械学習における差別」に取り組むための包括的な指針に向けた検討を行う。

\subsection{暗箱のリバースエンジニアリング}
先に述べたとおり、機械学習における差別の特異性は、個々のケースの「外面上」の独立性である。包括的な対処を行うためには、それぞれのケースにおける対策を統合するための指針が必要である。すでに見たように、差別の表れには不公平な配分「配分型」、承認に対する攻撃「象徴型」の二極があり、それぞれのケースにおいて、一見すると、それらの原因も取り得る対策も異なっているように見える。
このような事態において問題に対する対策の指針を統合しようとするならば、フレイザーの着眼に倣うのは意義深い。
すなわち、改善策を\.組\.み\.換\.え\.る\.視\.点で、包括的な対策の指針を見出そうとする試みが有効であると考えられる。


「組み換える視点」とは、機械学習において何らかの「不公正な配分」が出力における「象徴的な差別」を生み、何らかの「象徴的な差別」が出力における「不公正な配分」を生み出す構造を特定しようとする視点のことであると定義する。この視点に従った指摘とは、例えば、二章の「瞬目検知」のケースにおいて、学習やテストに用いた顔のデータセットの偏りが製品において「象徴的な差別」を引き起こしているという可能性を指摘することである。
また、「採用システム」において、学習されたネットワークの内部に特定の属性に対する「象徴的な差別」がエンコードされていると指摘することである。


このような分析は、
公平性を確保するという観点において、出力を直接制御することのない指標を定義することを可能にするかもしれない。
また、入力におけるある属性がどのような仕方で他の情報と結び付けられているのかを公平性の視点から分析することで、各属性が適切な承認を受けているのかを判別するための指標を得られる可能性がある。例えば、他の情報と組み合わせられることなく出力に影響を与えている属性は、なんらかの誤承認と結びついている可能性が高い。その偏りは、エントロピーなどの指標を用いることで数値化できると考えられる。
このような統合されたアプローチは、その仕様が公開されたときの影響を考慮すると、差異を特定して是正するという単純な方策よりも優れている。
そして、このような検討を重ねることで、個々のシステムが運用される社会経済的な背景が自ずと主題化されるだろう。すなわち、データセットが構築される過程はどのうようなものであるのか、あるいは個々のシステムが資本主義の力場においてどのような影響力を持っているのかについての議論が促進される。


さらに、この「組み換える視点」を階層的に行使することで、例えば「不公正な配分」の原因となった「象徴的な差別」を引き起こしている「不公正な配分」を特定することができる。このような研究は、現に差別を学習したシステムに対して「リバース・エンジニアリング」を行うこということを意味する\footnote{このようなリバースエンジニアリングの試みはすでに行われ始めている。\cite{Tan_2018} https://ascii.jp/elem/000/001/584/1584663/ 等を参照}。「箱庭の中」の秩序は、我々外部の社会のなかで生み出されたデータから学習される。よって、このような研究は「箱庭の外」の社会に対しても何らかの示唆を与える可能性がある。


本節では、フレイザーの「\.改\.善\.策\.の\.組\.み\.換\.え」の視点に基づいて、機械学習における差別を是正するための包括的な戦略を立てる上で必要と考えられる現状の差別の分析方法の提案を試みた。
ここでの検討は、あくまで抽象的な提案を行うにとどまっている。より具体的な考察を行うことは今後の課題である。
%さらに、その積み重ねにより、機械学習における「配分型」の差別と「象徴型」の差別の両者に共通する、ある共起構造を発見できる可能性がある。

%この取り組みは、実際のところ、「機械学習における差別」を超えた示唆を与える可能性がある。人の高次の





%https://ascii.jp/elem/000/001/584/1584663/
%何が決定的に違うか　承認



%第一に、原因と対策を区別する必要性 対策しながら原因を探すための方策が必要である点

%まだまだ議論することがある
%権力の中である差別の正当化するためにＡＩが使われることもあるはず
%でも最後は前向きな示唆を提示して〜

\section{\tbaselineshift=3.0pt おわりに：「暗---箱庭」と共に}
本稿を通じて、機械学習という「ブラックボックス」の中で生じる差別についてその内外から考察を行ってきた。本稿で検討した論点の他にも、議論すべきことはたくさんある。例えば、ある経済的あるいは政治的背景において、ある差別を自然化するために「ＡＩによる意思決定」という標語が利用されることもあり得るだろう。しかし、本稿で行った提案に基づく議論は、そのような状況にも抗うことが可能になるような枠組みを整備していくためにも不可欠であると考えられる。哲学の対象としての社会はある種のブラックボックスであり、その内外に働く政治的な力場を整理するために、様々な議論が行われてきた。ＡＩに代表されるような、機械学習を応用したシステムは、ある場合には我々の社会の何らかの構造を写し取る「箱庭」になる。それゆえ、ＡＩの社会実装を考える上では、政治的な運動についてのあらゆる哲学が考慮される必要がある。本稿の考察はあくまで予備的なものに留まっており、具体的な指針やその有効性をはっきりと示すには至っていない。しかし、我々は現代の社会においてＡＩという「暗―箱庭」と共に生きていくしかないのであり、これは必要な議論であるのだ。



\small
\begin{thebibliography}{99}
\small
  \bibitem{kate} Sidney Fussell. AI Professor Details Real-World Dangers of Algorithm Bias [Corrected] , 2012/08/17. \scriptsize \vspace{-1.5mm} \url{https://gizmodo.com}\\ \url{/microsoft-researcher-details-real-world-dangers-of-algo-1821129334}\small
  \bibitem{compas}Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner. Machine Bias, \textsl{ProPublica}. 2016/05/23.\vspace{-1.5mm} \\ \scriptsize \url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-}\vspace{-1.5mm} \\ \url{sentencing}\small
  \bibitem{Calders2010}Toon Calders and Sicco Verwer. Three naive Bayes approaches for discrimination-free classification, \textsl{Data Mining and Knowledge Discovery}, 2010/09/01, vol.21, number.2 pp.277--292
  \bibitem{DBLP:journals/corr/abs-1301-6822}Latanya Sweeney. Discrimination in Online Ad Delivery, \textsl{CoRR}, 2013, vol.abs/1301.6822,  \scriptsize \url{http://arxiv.org/abs/1301.6822}\small
  \bibitem{articleZou}James Zou and Londa Schiebinger. AI can be sexist and racist —it’s time to make it fair, \textsl{Nature}, 2018/07, vol.559. pp.324--326
  \bibitem{shankar2017classification}Shreya Shankar and Yoni Halpern and Eric Breck and James Atwood and Jimbo Wilson and D. Sculley. No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World, 2017
  \bibitem{inproceedings}Judd Antin and Raymond Yee and Coye Cheshire and Oded Nov. Gender differences in Wikipedia editing, \textsl{WikiSym 2011 Conference Proceedings - 7th Annual International Symposium on Wikis and Open Collaboration} 2011/10. pp.11--14
  \bibitem{Difallah2018DemographicsAD}Djellel Eddine Difallah and Elena Filatova and Panagiotis G. Ipeirotis. Demographics and Dynamics of Mechanical Turk Workers, \textsl{WSDM}. 2018
  \bibitem{obama2}Executive Office of the President May 2016. Big Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights, 2016/05. \vspace{-1.5mm} \scriptsize \url{https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf}\small
  \bibitem{obama}Executive Office of the President May 2014. BIG DATA:SEIZING OPPORTUNITIES, PRESERVING VALUES, 2016/05.  \vspace{-1.5mm} \scriptsize \url{https://obamawhitehouse.archives.gov/sites/default/files/docs/}\vspace{-1.5mm}\\ \url{big_data_privacy_report_may_1_2014.pdf}\small
  \bibitem{hardt2016equality}Moritz Hardt and Eric Price and Nathan Srebro. Equality of Opportunity in Supervised Learning, 2016
  \bibitem{mouzannar2018fair}Hussein Mouzannar and Mesrob I. Ohannessian and Nathan Srebro. From Fair Decision Making to Social Equality, 2018.
  \bibitem{dwork2011fairness}Cynthia Dwork and Moritz Hardt and Toniann Pitassi and Omer Reingold and Rich Zemel. Fairness Through Awareness, 2011.
  \bibitem{BB10510230}Nancy Fraser and Axel Honneth. 加藤 泰史、高畑 祐人、菊地 夏野、舟場 保之、中村 修一、遠藤 寿一、直江 清隆、『再配分か承認か？ : 政治・哲学論争』、法政大学出版局、叢書・ウニベルシタス、二〇一二
  
\end{thebibliography}


%\theendnotes

%\end{multicols}

\bibliographystyle{jplain}
%\bibliography{innyo}






\end{document}